---
layout: post
title:  "面试准备-Spark题目"
date:   2021-01-27 00:10:01
categories: 面试
---


# Spark题目


1. Spark Streaming如何实现实时计算？
- Spark Streaming是基于Spark Core基础上，用于处理实时计算业务的框架。其原理是将输入的流数据按时间进行切分，划分为离散的数据块DStream；然后将每个微小的批数据转化为RDD，并使用RDD算子进行处理，切分的数据块用离线批处理的方式进行并行计算，处理后的结果也以RDD的形式返回；
- 因此，Spark Streaming本质上还是一种针对微小的批数据的离线处理方法，只是增加了一层将收集的数据按照时间节点划分的操作；
- Spark Streaming框架有两个重要角色：Driver和Executor
    - Driver：作为Spark Application的master，负责任务执行计划生成和任务分发、调度；
    - Executor：作为Spark Application的worker，负责实际执行任务的task，并将计算的执行结果返回给Driver；
    - 从流程上看，Spark Streaming获取外部数据源（例如MQ）之后，内部依靠一个时钟Clock，按batch interval来对数据进行定时分片，然后将每一个batch interval内的数据提交给task进行逻辑处理；


---
2. 现在有这样一种业务场景，需要统计过去两小时的放款量，并进行实时统计，应该构建哪种计算框架？（微众面试）
- 使用Spark Streaming的滑动窗口进行计算；
- 滑动窗口windowDuration的功能是，对过去一个时间间隔内的数据进行合并生成一个RDD，以便于对该时间间隔内的数据进行统计计算；
- 滑动窗口由两部分组成
    - window length: 窗口计算的时间跨度。指本次window操作所包含的过去的时间间隔，为batch interval的整数倍；
    - sliding interval: 窗口计算的执行频率。指每隔多长时间计算一次，也为batch interval的整数倍；
- 因此，在这种业务情景下，window length设置为2小时，转换为batch interval的若干倍；slide inerval设置为预计的播报间隔，例如1小时计算一次，同样也转换为batch interval的若干倍设置。这样可以获得最近两小时内的业务数据对应的RDD，再进行简单的加总计算即可；


---
3. Spark的窗口函数（又可以称为开窗函数）是什么？
- ranking 排名类
    - `row_number() over (partition by [col_1] order by [col_2])`，很单纯的行号，类似excel的行号
    - `rank() over (partition by [col_1] order by [col_2])`，rank值可能是不连续的，例如重复
    - `dense_rank() over (partition by [col_1] order by [col_2])`，rank值一定是连续的
    - `percent_rank() over (partition by [col_1] order by [col_2])`，以百分比形式给出排序效果，范围为[0, 1]
    - `cume_dist() over (partition by [col_1] order by [col_2])`，以百分比形式给出排序效果，范围为(0, 1]
    - `ntile([num]) over (partition by [col_1] order by [col_2])`，将制定的数据平均分配到指定数量的桶中，桶的index从1开始
- analytic 分析类
    - `first_value() over (partition by [col_1] order by [col_2])`，获取分组内的第一个值
    - `last_value() over (partition by [col_1] order by [col_2])`，获取分组内的最后一个值
    - `lag([col], [num]) over (partition by [col_1] order by [col_2])`，在窗口内往上取第二个值
    - `lead([col], [num]) over (partition by [col_1] order by [col_2])`，在窗口内往下取第二个值
- aggregate 聚合类
    - `sum() over (partition by [col_1] order by [col_2])`
    - `count() over (partition by [col_1] order by [col_2])`
    - `avg() over (partition by [col_1] order by [col_2])`
    - `max() over (partition by [col_1] order by [col_2])`
    - `min() over (partition by [col_1] order by [col_2])`


---
4. Spark的容错机制是什么？（货拉拉面试）
- Spark的容错，指的是Spark可以在一定范围内，包容错误的发生，并使得计算继续下去的特性；
- Spark的计算是基于Driver和Executor进行的，因此其容错也可以分为Driver的容错和Executor的容错；
- 当Executor挂掉之后，Driver通过监听获取失败信息，然后自动寻找下一个Executor；LineAge记录了RDD的操作行为，由于数据会有备份，因此新的Executor直接读取备份数据重新计算；Executor的容错几乎不需要人工的介入；
- 当Driver挂掉之后，所有的Executor都会失效；Driver的容错是基于CheckPoint机制进行的，它会定期的将Driver中的信息写入到HDFS中，从而使得Driver挂掉的时候快速的读取运行信息、并自动恢复，接着上一次的结果进行计算。在YARN中可以设置自动重启Driver的次数；


---
5. Spark为什么比MapReduce快？（货拉拉面试）
- Spark和MapReduce的计算都发生在内存当中，但前者的速度确实更快。这是因为，MapReduce需要将计算的中间结果写入磁盘，然后还要读取磁盘，从而导致了频繁的磁盘IO；而Spark则不需要将中间结果大量的写入磁盘，这得益于Spark的RDD和DAG，其中，RDD将中间结果存储在内存中，DAG记录了父子RDD的依赖关系，从而大大减少了磁盘的读写；
- 二者的shuffle过程有所不同。MapReduce的shuffle不可避免的会进行排序的工作，而排序会花费大量的时间；而Spark的shuffle只有部分场景需要排序，且支持基于hash的分布式聚合，因此更加省时；
- MapReduce采用多进程模型，其优点是便于细粒度的控制每个任务的资源，但每个map和reduce任务的启动都需要开启JVM进程，从而消耗了更多的时间；而Spark采用多线程模型，通过复用线程池的线程来减少启动、关闭task所需要的开销，但其缺点在于，同一个节点上的所有任务都运行在一个进程中，因此会产生严重的资源争夺现象，难以细粒度的控制每个任务的资源占用情况；


---
6. Spark的内存模型是什么样的？（阿里巴巴面试，好难理解）
- Spark能够有效的利用内存进行分布式计算，其内存管理模块在整个系统中扮演着非常重要角色。Spark的内存管理，分为堆内内存，和堆外内存；
- 堆内内存(On-heap Memory)。其中前三类被称为usable Memory， usableMemory = systemMemory - reservedMemory
    - Execution内存：主要用于存放shuffle/ join/ sort等计算过程中的临时数据；
    - Storage内存：主要用于存储Spark的缓存数据，例如RDD的缓存；
    - User内存：主要用于存储RDD转换操作所需要的数据，例如RDD依赖等信息；
    - Reserved内存：系统预留内存，会用来存储Spark内部对象；
- 堆外内存(Off-heap Memory)。堆外内存不在JVM中申请内存，而是调用了Java的unsafe API，直接向操作系统申请内存；由于这种方式不需要经过JVM的内存管理，因此可以避免频繁的垃圾回收。但这种内存管理的缺点是，必须自己编写内存申请和释放的逻辑。堆外内存可以分为两种，DirectMemory/ JVM Memory。默认情况下，堆外内存是关闭的；当手动启用堆外内存时，那么Execution内存会同时在堆内和堆外存在，二者的使用互不影响。
    - Execution内存：
    - Storage内存：


---
1. Spark数据分发、任务分发机制是什么？


---
5. spark partition的类型及特点


---
6. Spark rdd特性和理解


---
7. Spark的大致运行流程？

























4. Spark的有几种部署模式？每种模式的特点是什么？（整理来源：《Spark面试2000题》）
- 本地模式。Spark不一定非要跑在Hadoop集群，可以在本地用多线程的方式来指定。优点是方便调试，本地模式分三类
    - local：只启动一个executor
    - local[k]：启动k个executor
    - local：启动跟CPU数目相同的executor
- standalone模式。分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础
- Spark on yarn模式。分布式部署集群，资源和任务监控交给yarn管理；粗粒度资源分配方式，包含cluster和client运行模式
    - cluster：适合生产，Driver运行在集群子节点，具有容错功能
    - client：适合调试，dirver运行在客户端
- Spark On Mesos模式


---
2. Spark技术栈有哪些组件？每个组件都有什么功能？适合什么应用场景？
- Spark core。是其它组件的基础，Spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等
- SparkStreaming。是一个对实时数据流进行高通量、容错处理的流式处理系统，将流式计算分解成一系列短小的批处理作业
- Spark SQL。能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询
- MLBase。是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime
- GraphX。是Spark中用于图和图并行计算


---
3. Spark有哪些组件？
- Master：管理集群和节点，不参与计算
- worker：计算节点，进程本身不参与计算，和Master汇报
- Driver：运行程序的main方法，创建Spark context对象
- Spark context：控制整个application的生命周期，包括dagsheduler和task scheduler等组件
- client：用户提交程序的入口


---
4. Spark工作机制
- 用户在client端提交作业后，会由Driver运行main方法并创建Spark context上下文。
- 执行add算子，形成dag图输入dagscheduler
- 按照add之间的依赖关系划分stage输入task scheduler
- task scheduler会将stage划分为taskset分发到各个节点的executor中执行


---
5. Spark应用程序的执行过程
- 构建Spark Application的运行环境（启动SparkContext）
- SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；
- 资源管理器分配Executor资源，Executor运行情况将随着心跳发送到资源管理器上；
- SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler
- Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行，SparkContext将应用程序代码发放给Executor。
- Task在Executor上运行，运行完毕释放所有资源；


---
6. Driver的功能是什么？
- 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的人口点；
- 功能：向集群申请资源、负责了作业的调度和解析、生成Stage并调度Task到Executor上（包括DAGScheduler，TaskScheduler）；


---
7. Spark中Work的主要工作是什么？
- 管理当前节点内存，CPU的使用状况，接收Master分配过来的资源指令，通过ExecutorRunner启动程序分配任务
- worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务
- worker不会运行代码，具体运行的是Executor，是可以运行具体appliaction写的业务逻辑代码


---
8. task有几种类型？
- resultTask类型，最后一个task；
- shuffleMapTask类型，除了最后一个task都是；


---
9. 什么是shuffle，以及为什么需要shuffle？
- shuffle中文翻译为洗牌
- 需要shuffle的原因是：某种具有共同特征的数据，汇聚到一个计算节点上进行计算


---
10. Spark Master HA 主从切换过程不会影响集群已有的作业运行，为什么？
- 因为程序在运行之前，已经申请过资源了，Driver和Executors通讯，不需要和Master进行通讯的；


---
11. Spark并行度怎么设置比较合适？
- Spark并行度，每个core承载2~4个partition，即并行度
- 并行读和数据规模无关，只和内存和CPU有关


---
12. Spark程序默认执行时，为什么会产生很多task，怎么修改默认task执行个数？
- 有很多小文件的时候，有多少个输入block就会有多少个task启动
- Spark中有partition的概念，每个partition都会对应一个task，task越多，在处理大规模数据的时候，就会越有效率


---
13. Spark中数据的位置是被谁管理的？
- 每个数据分片都对应具体物理位置，数据的位置是被blockManager管理


---
14. 为什么要进行序列化？
- 减少存储空间，高效存储和传输数据
- 缺点：使用时需要反序列化，非常消耗CPU


---
15. Spark如何处理不能被序列化的对象？
- 封装成object


---
16. Spark提交你的jar包时所用的命令是什么？
- `Spark-submit`


---
17. MapReduce和Spark的相同和区别？
- 两者都是用MR模型来进行并行计算
- Hadoop的一个作业：job，job分为map task和reduce task，每个task都是在自己的进程中运行的当task结束时，进程也会结束
- Hadoop的job只有map和reduce操作，表达能力比较欠缺
- 在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系
- Spark用户提交的任务：application，一个application对应一个Sparkcontext，app中存在多个job，每触发一次action操作就会产生一个job，这些job可以并行或串行执行
- 每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的
- 每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行
- executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。
- Spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，通过DAG图可以实现良好的容错


---
18. 简答说一下Hadoop的MapReduce编程模型？
- 首先map task会从本地文件系统读取数据，转换成key-value形式的键值对集合，使用的是Hadoop内置的数据类型（longwritable、text）
- 将键值对集合输入mapper进行业务处理过程，将其转换成需要的key-value再输出
- 之后会进行一个partition分区操作，默认使用的是hashpartitioner，自定义分区：重写getpartition方法
- 之后会对key进行进行sort排序，grouping分组操作将相同key的value合并分组输出
- 之后进行一个combiner归约操作，其实就是一个本地段的reduce预处理，以减小后面shufle和reducer的工作量
- reduce task会通过网络将各个数据收集进行reduce处理
- 最后将数据保存或者显示，结束整个job


---
19. 简单说一下Hadoop和Spark的shuffle相同和差异？
- high-level 角度：两者并没有大的差别 都是将 mapper（Spark: ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce()
- low-level 角度：Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。
好处：combine/reduce() 可以处理大规模的数据
因为其输入数据可以通过外排得到
mapper 对每段数据先做排序
reducer 的 shuffle 对排好序的每段数据做归并
Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不提前排序
如果用户需要经过排序的数据：sortByKey()
实现角度：
Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spilt, merge, shuffle, sort, reduce()
Spark 没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，spill, merge, aggregate 等操作需要蕴含在 transformation() 中


---
20. 简单说一下Hadoop和Spark的shuffle过程
- Hadoop：map端保存分片数据，通过网络收集到reduce端
- Spark：Spark的shuffle是在DAGSchedular划分Stage的时候产生的，TaskSchedule要分发Stage到各个worker的executor，减少shuffle可以提高性能


---
partition和block的关联
hdfs中的block是分布式存储的最小单元，等分，可设置冗余，这样设计有一部分磁盘空间的浪费，但是整齐的block大小，便于快速找到、读取对应的内容
Spark中的partition是RDD的最小单元，RDD是由分布在各个节点上的partition组成的。
partition是指的Spark在计算过程中，生成的数据在计算空间内最小单元
同一份数据（RDD）的partion大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定
block位于存储空间；partion位于计算空间，block的大小是固定的、partion大小是不固定的，是从2个不同的角度去看数据。


---
Spark为什么比MapReduce快？
基于内存计算，减少低效的磁盘交互
高效的调度算法，基于DAG
容错机制Linage


---
MapReduce操作的mapper和reducer阶段相当于Spark中的哪几个算子？
相当于Spark中的map算子和reduceByKey算子，区别：MR会自动进行排序的，Spark要看具体partitioner


---
RDD机制
分布式弹性数据集，简单的理解成一种数据结构，是Spark框架上的通用货币
所有算子都是基于rdd来执行的
rdd执行过程中会形成dag图，然后形成lineage保证容错性等
从物理的角度来看rdd存储的是block和node之间的映射

---
RDD的弹性表现在哪几点？
自动的进行内存和磁盘的存储切换；
基于Lingage的高效容错；
task如果失败会自动进行特定次数的重试；
stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；
checkpoint和persist，数据计算之后持久化缓存
数据调度弹性，DAG TASK调度和资源无关
数据分片的高度弹性，a.分片很多碎片可以合并成大的，b.par

---
RDD有哪些缺陷？
不支持细粒度的写和更新操作（如网络爬虫）
Spark写数据是粗粒度的，所谓粗粒度，就是批量写入数据 （批量写）
但是读数据是细粒度的也就是说可以一条条的读 （一条条读）
不支持增量迭代计算，Flink支持


---
什么是RDD宽依赖和窄依赖？
RDD和它依赖的parent RDD(s)的关系有两种不同的类型
窄依赖：每一个parent RDD的Partition最多被子RDD的一个Partition使用 （一父一子）
宽依赖：多个子RDD的Partition会依赖同一个parent RDD的Partition （一父多子）


---
cache后面能不能接其他算子,它是不是action操作？

可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache
cache不是action操作

---
什么场景下要进行persist操作？
以下场景会使用persist

某个步骤计算非常耗时或计算链条非常长，需要进行persist持久化
shuffle之后为什么要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大
shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。

---
rdd有几种操作类型？三种！！
transformation，rdd由一种转为另一种rdd
action
cronroller，控制算子(cache/persist) 对性能和效率的有很好的支持


---
reduceByKey是不是action？
不是，很多人都会以为是action，reduce rdd是action


---
collect功能是什么，其底层是怎么实现的？
Driver通过collect把集群中各个节点的内容收集过来汇总成结果
collect返回结果是Array类型的，合并后Array中只有一个元素，是tuple类型（KV类型的）的。


---
map与flatMap的区别
map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象
flatMap：对RDD每个元素转换，然后再扁平化，将所有的对象合并为一个对象，会抛弃值为null的值


---
列举你常用的action？
collect，reduce,take,count,saveAsTextFile等


---
union操作是产生宽依赖还是窄依赖？
窄依赖


---
Spark累加器有哪些特点？
全局的，只增不减，记录全局集群的唯一状态
在exe中修改它，在Driver读取
executor级别共享的，广播变量是task级别的共享
两个application不可以共享累加器，但是同一个app不同的job可以共享


---
Spark hashParitioner的弊端
分区原理：对于给定的key，计算其hashCode
弊端是数据不均匀，容易导致数据倾斜


---
RangePartitioner分区的原理
尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大
分区内的元素是不能保证顺序的
简单的说就是将一定范围内的数映射到某一个分区内


---
Spark中的HashShufle的有哪些不足？
shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；
容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息
容易出现数据倾斜，导致OOM


---
如何使用Spark解决TopN问题？（互联网公司常面）
https://blog.csdn.net/oopsoom/article/details/25815443


---
如何使用Spark解决分组排序问题？（互联网公司常面）、
https://blog.csdn.net/huitoukest/article/details/51273143


---
给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url?
方案1：可以估计每个文件安的大小为5G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。
遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件(记为a0,a1,…,a999)中。这样每个小文件的大约为300M。
遍历文件b，采取和a相同的方式将url分别存储到1000小文件(记为b0,b1,…,b999)。这样处理后，所有可能相同的url都在对应的小文件(a0vsb0,a1vsb1,…,a999vsb999)中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
求每对小文件中相同的url时，可以把其中一个小文件的url存储到hash_set中。然后遍历另一个小文件的每个url，看其是否在刚才构建的hash_set中，如果是，那么就是共同的url，存到文件里面就可以了。
方案2：如果允许有一定的错误率，可以使用Bloomfilter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloomfilter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloomfilter，如果是，那么该url应该是共同的url(注意会有一定的错误率)。


---
有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M，要求返回频数最高的100个词。
Step1：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件(记为f0,f1,...,f4999)中，这样每个文件大概是200k左右，如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M;
Step2：对每个小文件，统计每个文件中出现的词以及相应的频率(可以采用trie树/hash_map等)，并取出出现频率最大的100个词(可以用含100个结点的最小堆)，并把100词及相应的频率存入文件，这样又得到了5000个文件;
Step3：把这5000个文件进行归并(类似与归并排序);


---
现有海量日志数据保存在一个超级大的文件中，该文件无法直接读入内存，要求从中提取某天出访问百度次数最多的那个IP。
分而治之+Hash
1)IP地址最多有2^32=4G种取值情况，所以不能完全加载到内存中处理;
2)可以考虑采用“分而治之”的思想，按照IP地址的Hash(IP)%1024值，把海量IP日志分别存储到1024个小文件中。这样，每个小文件最多包含4MB个IP地址;
3)对于每一个小文件，可以构建一个IP为key，出现次数为value的Hashmap，同时记录当前出现次数最多的那个IP地址;
4)可以得到1024个小文件中的出现次数最多的IP，再依据常规的排序算法得到总体上出现次数最多的IP;


---
在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。
方案1：采用2-Bitmap(每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义)进行，共需内存2^32*2bit=1GB内存，还可以接受。然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。
方案2：也可采用与第1题类似的方法，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。


---
腾讯面试题：给40亿个不重复的unsignedint的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中?
申请512M的内存，一个bit位代表一个unsignedint值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。


---
**1. mllib支持的算法？ **

分类、聚类、回归、协同过滤


---
kmeans算法原理

随机初始化中心点范围，计算各个类别的平均值得到新的中心点。
重新计算各个点到中心值的距离划分，再次计算平均值得到新的中心点，直至各个类别数据平均值无变化。


---
朴素贝叶斯分类算法原理
对于待分类的数据和分类项，根据待分类数据的各个特征属性，出现在各个分类项中的概率判断该数据是属于哪个类别的。


---
关联规则挖掘算法apriori原理
一个频繁项集的子集也是频繁项集，针对数据得出每个产品的支持数列表，过滤支持数小于预设值的项，对剩下的项进行全排列，重新计算支持数，再次过滤，重复至全排列结束，可得到频繁项和对应的支持数。


---
**1. Hive中存放是什么？ **
表（数据+元数据） 存的是和hdfs的映射关系，hive是逻辑上的数据仓库，实际操作的都是hdfs上的文件，HQL就是用sql语法来写的mr程序。


---
Spark服务端口
8080 Spark集群web ui端口
4040 Sparkjob监控端口
18080 jobhistory端口


---
Spark Job 默认的调度模式 - FIFO
RDD 特点 - 可分区/可序列化/可持久化
Broadcast - 任何函数调用/是只读的/存储在各个节点
Accumulator - 支持加法/支持数值类型/可并行
Task 数量由 Partition 决定
Task 运行在 Workder node 中 Executor 上的工作单元
Master 和 worker 通过 Akka 方式进行通信的
默认的存储级别 - MEMORY_ONLY
hive 的元数据存储在 derby 和 MySQL 中有什么区别 - 多会话
DataFrame 和 RDD 最大的区别 - 多了 schema


---
1,Spark的工作机制

用户在客户`端提交作业后，会由Driver运行main方法并创建SparkContext上下文,
SparkContext向资源管理器申请资源, 启动Execotor进程, 并通过执行rdd算子，形成DAG有向无环图,输入DAGscheduler, 然后通过DAGscheduler调度器, 将DAG有向无环图按照rdd之间的依赖关系划分为几个阶段,也就是stage, 输入task scheduler, 然后通过任务调度器taskscheduler将stage划分为task set分发到各个节点的executor中执行。

2,Spark中stage是如何划分的

在DAG调度的过程中，Stage阶段的划分是根据是否有shuffle过程，也就是存在ShuffleDependency宽依赖的时候，需要进行shuffle,这时候会将作业job划分成多个Stage

整体思路：从后往前推，遇到宽依赖就断开，划分为一个 stage；遇到窄依赖就将这个 RDD 加入该 stage 中

3, Spark的shuffle和调优

Spark中一旦遇到宽依赖就需要进行shuffle的操作,本质就是需要将数据汇总后重新分发的过程, 也就是数据从map task输出到reduce task输入的这段过程, 在分布式情况下，reduce task需要跨节点去拉取其它节点上的map task结果,这就要数据落磁盘,会影响性能,所以说Spark通过流水线优化尽力的避免shuffle操作来提高效率
Spark的shuffle总体而言就包括两个基本的过程:Shuffle write 和Shuffle read
在发生shuffle操作的算子中,需要进行stage的划分,shuffle操作之前的一个stage称之为map task,shuffle操作后的一个stage称之为reduce task, 其中map task负责数据的组织，也就是将同一个key对应的value根据hash算法写入同一个下游task对应的分区文件中,也就是Shuffle write, 其中reduce task负责数据的聚合，也就是在上一个stage的task所在的节点上，把属于自己的各个分区文件都拉取过来进行聚合，这个过程称之为shuffle read
map task会将数据先保存在内存中，如果内存不够时，就溢写到磁盘文件中，reduce task 会读取各个节点上属于自己的分区磁盘文件到自己节点的内存中进行聚合
Spark shuffle参数调优:
1, Spark.shuffle.file.buffe: 32k,参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘
2, Spark.reducer.maxSizeInFlight:48m 该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据
3, Spark.shuffle.io.maxRetries:3, shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败

4, SparkStreaming调优

1,资源参数调优
增加Driver和Executor的内存,设置合理的CPU个数(--num-executors ,--executor-cores通常设置较多的Executor个数和较少的Executor core个数来达到资源最大使用率)
2,增加并行度parallelism:增加Spark partition数量
Spark.default.parallelism 可以设置Spark 默认的分区数量,另外,在SparkStreaming+Kafka的案列中,如果采用Direct的方式从Kafka中获取数据,此时Kafka partition的数量和Spark Rdd的分区数量是1:1的映射关系,最好的方式是直接增加Kafka的分区个数,比如提高到100或者更高;还有一种方式,是可以在创建InputDstream之后再repartition一个更大的并行度,然后再进行逻辑计算,但是实际操作中性能提升很小,是因为repartition会造成shuffle操作
3,设置合理的批处理时间和Kafka数据拉取速率maxRatePerPartition
Spark会每隔batchDuration时间去提交一次job(让程序去处理),如果job处理时间超过了batchDuration的设置,那么会导致后面的作业无法按时提交,随着时间推移,越来越多的作业被拖延,导致整个streaming作业被阻塞
批处理时间不能太短,如果每个批次产生的job不能再这个时间内处理完成,就会造成数据不断积压
另外根据生产者写入 Kafka 的速率以及 Streaming 本身消费数据的速率设置合理的 Kafka 读取速率（Spark.streaming.kafka.maxRatePerPartition），使得 Spark Streaming 的每个 task 都能在 Batch 时间内及时处理完 Partition 内的数据，使 Scheduling Delay 尽可能的小
4,使用Kryo序列化
SparkStreaming官方在调优的时候第一个建议就是设置序列化
SparkStreaming在传输,使用对象的时候需要使用序列化和反序列化,Spark默认的是使用Java内置的序列化类, 官方介绍，Kryo序列化机制比Java序列化机制，性能高10倍左右
Spark之所以默认没有使用Kryo作为序列化类库，是因为Kryo要求最好只要是自定义类型,都需要进行注册，因此对于开发者来说，这种方式比较麻烦
5,缓存需要经常使用的数据
对一些经常使用到的数据，我们可以显式地调用rdd.cache()来缓存数据，这样也可以加快数据的处理，但是我们需要更多的内存资源
6,Spark优化之广播变量
使用广播变量在SparkContext中，可以大幅降低每一个序列化task这个对象的大小，集群中启动一个job的成本也会降低。如果你的task中使用了一个大对象(large object)，考虑把他优化成一个广播变量。通常来说，一个task大于20KB就值得优化

5,Spark的算子和分类

Spark算子一般分为TractionFormation类型的算子和Action类型的算子
1,Transformaction:这种变换并不触发提交作业,而是完成作业中间的处理. TractionFormation操作是懒执行的,也就是从一个Rdd转化为另一个Rdd的转化并不是马上执行,而是需要等到有Action操作的时候才会真正触发运算,一般是Map等方法
2,Action行动算子:这类算子会触发SparkContext提交job作业,并将数据落地到磁盘,进行shuffle,对性能开销比较大
Shuffle(Action)算子分类
去重 : def distinct()
聚合 : def reduceByKey() def groupByKey() aggregateByKey combinByKey()
排序 : def sortByKey() def sortBy()
重分区 : def reparation() ==def coalesce() [ˌkəʊəˈles]扩大或者缩小分区
集合或者表操作: join 内连接,intersection 交集 subtract 差集

6,Spark数据倾斜 shuffle操作问题解决(也可以作为遇到的问题)

1, 首先要知道发生数据倾斜的原理,数据倾斜就是进行shuffle的时候,必须将各个节点上相同的key拉取到某个节点上的一个task来进行处理,此时如果某个key对应的数据量特别大的话,就会发生数据倾斜
2, 数据倾斜问题发现和定位,通过Spark web ui 来查看当前运行的stage各个task分配的数据量,从而进一步确定是不是task分配的数据不均匀导致了数据倾斜,知道数据倾斜发生在哪个stage之后,我们就需要根据stage划分原理,推算出来发生倾斜的那个stage对应代码中的哪一部分,这部分代码中肯定会有一个shuffle类型的算子,通过countByKey查看各个key的分布.
3,数据倾斜的 解决方案
(1),过滤少数导致倾斜的key
(2),提高shuffle操作的并行度
(3),如果是聚合类算子,使用局部聚合和全聚合的方式,下面是思路
方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。

(4),如果是join类型的算子,则使用随机前缀和扩容RDD进行join
(RDD中有大量的key导致数据倾斜)
方案实现思路：将含有较多倾斜key的RDD扩大多倍，与相对分布均匀的RDD配一个随机数。

7,遇见的问题 1,序列化,2,数据倾斜 见上面

报错:Serialization stack: Task not serializable
当我们要对Rdd做map,flapMap,filter等操作的时候是在excutor上完成的,当我们在Driver中定义了一个变量,在Rdd中使用的时候,这个变量就需要分发到各个excutor上去,因为dirver和executor运行在不同的jvm中,这就需要涉及对象的序列化和反序列化,如果这个变量没有被序列化,就会报这个异常;
我所遇见的,有来自第三方的一个类对象定义在Driver端,但是他没有实现序列化的接口java.io.Serializable,而我又没法去修改它的源码,这时候我就想到自己定义一个类去继承这个第三方类,然后让自己定义的类实现java.io.serializable,然后再Rdd中调用自己的类,问题就得到了解决
另外的方式是将不可序列化的对象定义在闭包中,或者使用为其注册序列化类的方法;
8,Spark的特点
