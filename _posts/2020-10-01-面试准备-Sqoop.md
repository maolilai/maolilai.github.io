# Sqoop的面试题总结


1. Sqoop参数有哪些？
Sqoop是一种在Hadoop和关系型数据库之间传输数据的工具。Sqoop的命令用的最多的是
- `sqoop import`: RDBMS -> HDFS
- `sqoop export`: HDFS -> RDBMS
主要参数有如下所示：
- `--connect <url>`，指定RDMBS的IP. 端口；
- `--username`，指定RDBMS的用户名；
- `--password`，指定RDBMS的密码；
- `--table`，指定RDBMS的库名. 表名；
- `--driver`，指定JDBC连接数据库的驱动；
- `--hive-import`，指定导入数据到Hive；
- `--target-dir`，指定导入数据到HDFS；
- `--hive-database`，指定Hive的库名；
- `--hive-table`，指定Hive的表名；
- `--incremental append`，指定为增量导入；
- `--num-mappers`，指定map的数量；
- `--fields-terminated-by`，指定Hive表字段的分隔符；
- `--lines-terminated-by`，指定Hive行的分隔符；
- `--query`，指定SQL语句；


---
2. Sqoop导入导出Null存储如何处理？
- null在Hive中以'\N'字符来存储，在MySQL中以null来存储。因此，在导入数据到HDFS时，需要将null转化为'\N'，使用命令为
`-null-string "\N"`
`-null-non-string "\N"`

在导出数据到RDBMS时，需要将'\N'转化为null，使用命令为
`-input-null-string "\N"`
`-input-null-non-string "\N"`


---
3. Sqoop数据导出一致性问题如何解决（部分map任务失败）？
如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。
- 使用—staging-table选项，将hdfs中的数据先导入到辅助表中，当hdfs中的数据导出成功后，辅助表中的数据在一个事务中导出到目标表中（也就是说这个过程要不完全成功，要不完全失败）。
为了能够使用staging这个选项，staging表在运行任务前或者是空的，要不就使用—clear-staging-table配置，如果staging表中有数据，并且使用了—clear-staging-table选项,sqoop执行导出任务前会删除staging表中所有的数据。
注意：–direct导入时staging方式是不可用的，使用了—update-key选项时staging方式也不能用

sqoop export \
--connect url \
--username root \
--password 123456 \
--table app_cource_study_report \
--columns watch_video_cnt,complete_video_cnt,dt \
--fields-terminated-by "\t" \
--export-dir "/user/hive/warehouse/tmp.db/app_cource_study_analysi_${day}" \
--staging-table app_cource_study_report_tmp \
--clear-staging-table \
--input-null-string '\\N' \
--null-non-string "\\N"


---
4. Sqoop底层运行的任务时什么？import和export的步骤和原理是什么？
- Sqoop的导入和导出，都是只有Map阶段，没有Reduce阶段的任务；
- `Sqoop import` 数据导入步骤
    - Sqoop首先通过JDBC从RDBMS读取元数据，将RDBMS中的行、列、数据类型信息，映射为Java中的数据类型；Sqoop的代码生成器使用这些元数据来创建对应表的类，用于保存从表中抽取的信息；
    - 获取的元数据中，包含RDBMS中表的主键的信息；Hadoop提供的InputFormat方法会根据主键，将Sqoop作业划分为若干个map，并提交到集群上执行；
    - 每个map将自己的任务完成后，输出到文件；
- `Sqoop export` 数据导出步骤
    - Sqoop首先通过JDBC从RDBMS读取元数据，根据目标表的定义生成一个Java类；这个类可以解析HDFS中的文本文件，并向表中插入合适的格式和数值；
    - 接着启动MapReduce任务，从HDFS中读取元数据，使用该类进行解析、导出；
    - 基于JDBC的导出方法，会产生一批insert语句，每条语句都会向目标表中插入多条记录；


---
5. Sqoop数据导出一次执行多长时间？
- Sqoop任务5分钟-2个小时的都有。取决于数据量


---
6. RDBMS中的增量数据如何导入？
--check-column 字段名 \  #指定判断检查的依据字段
--incremental  导入模式\  # 用来指定增量导入的模式（Mode），append和lastmodified
--last-value 上一次导入结束的时间\
--m mapTask的个数 \
--merge-key 主键


---
7. Map task并行度设置大于1的问题？
并行度导入数据的 时候 需要指定根据哪个字段进行切分 该字段通常是主键或者是自增长不重复的数值类型字段，否则会报下面的错误。
Import failed: No primary key could be found for table. Please specify one with --split-by or perform a sequential import with ‘-m 1’.
那么就是说当map task并行度大于1时，下面两个参数要同时使用
–split-by id 指定根据id字段进行切分
–m n 指定map并行度n个


---
8. 从mysql导入到Hive出现数据带入错误：当字段中存在输入tab键，会被hive识别多创建一条字段，如何处理？
- sqoop import 语句中添加`--hive-drop-import-delims`来把导入数据中包含的hive默认的分隔符去掉。


---
9. Sqoop导出数据到MySQL，会发生字段名称问题吗？如何解决？
- sqoop 从 mysql 导入 hive 的字段名称问题 hive 中有些关键字限制，因此有些字段名称在 mysql 中可用，但是到了 hive 就不 行。部分不能在 hive 中使用的字段名称 order；sort；reduce；cast；directory


---
10. Sqoop同步数据的时候，会开启几个mapper和几个Reducer？
- Sqoop同步数据时，会根据`sqoop -m <mapper_num>`人为给定、或自动给定mapper的数量，同时它是一个只有Map、没有Reduce的MapReduce程序，因此Sqoop的Reducer数量是0；


---
11.  Sqoop如何加快同步速度？（重要）
- 影响Sqoop的同步速度的因素，及解决方法大致如下所述
- 原因一：数据源的读取速度。使用`--direct`参数，即使用直连的方式，加快传输速度，但目前仅对MySQL/ PostgreSQL有效，分别使用直连的MySQL_dump/ pg_dump；此外，也可以通过`sqoop -m <mapper_num>`参数控制并发数的方式，以加快读取速度；
- 原因二：HDFS的写入速度。需要保证Yarn分配给Sqoop的资源充足，不能让集群资源成为同步的瓶颈；
- 原因三：数据倾斜程度。Sqoop默认的导入策略，是根据主键进行分区的导入，具体的并发粒度取决于`-m`参数。如果主键值的分布非常不均匀，那么Sqoop同步数据的时候就会产生严重的数据倾斜现象。这种情况下需要使用`--split-by`参数按照字段进行预先切分；


---
12. 事务性如何理解？
- 在关系型数据库中，事务性是一种工作机制，即：作为单个逻辑单元执行的一系列操作，要么完整的执行，要么完全不执行，不会存在部分执行的情况。一个逻辑工作单元（例如一段SQL语句）要成为事务，必须满足所谓的ACID特性。使用事务机制的好处十分明显，例如，在银行转账的过程中，转账由两步数据库操作组成，从甲方账户中扣款1万元，同时从乙方账户中增加1万元。遵循事务机制，若两种同时成功，则转账行为成功；若两者同时失败，则转账行为失败。不可能存在扣款成功、增加余额失败的情况。
- 原子性Atomicity: 指事务包含的所有操作要么全部成功，要么全部失败并回滚，因此事务的操作如果成功就必须要完全应用到数据库，如果操作失败则不能数据库有任何影响。例如用户A账户金额的减少，和用户B账户金额的增加，这两种变化要么都发生，要么都不发生，不会出现用户A账户金额减少、而用户B账户金额不增加的情况；
- 一致性Consistency: 指事务必须使数据库从一个一致性状态变换到另一个一致性状态，也就是说一个事务执行之前和执行之后都必须处于一致性状态。拿转账来说，假设用户A和用户B两者的钱加起来一共是一万元，那么不管A和B之间如何转账，转几次账，事务结束后两个用户的钱相加起来应该还得是一万元，这就是事务的一致性；
- 隔离性Isolation: 指多个并发的事务同时访问一个数据库时，一个事务不应该被另一个事务所干扰，每个并发的事务间要相互进行隔离。即多个事务并发执行后的状态，和它们串行执行后的状态是等价的。例如，用户A向用户B转账两次，每次一万元，那么执行结果应该是其账户余额减少了2万元；
- 持久性Durability: 指一个事务一旦被提交了，那么对数据库中的数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。例如转账事务发生后，用户A的账户减少了一万元，用户B的账户增加了一万元，不会因为数据库故障，而将这种变化消除掉；


---
13.  Sqoop的事务性怎么理解？对于1条记录，使用1个事务去进行数据同步，合理吗？（重要）
- `sqoop export`导出数据往往是并行的，导致导出操作往往不是原子操作，并且RDBMS使用固定大小的缓冲区来存储事务数据；
- `--staging-table <table_name>`参数可以用来保证同步数据过程中事务的安全性，因为在导入的过程中可能会有多个事务，一个事务的失败会影响到整体的同步情况；该参数可以创建一个与目标表同样的数据结构，所有的事务结果会先存放在该表中，然后又该表通过一次性事务将结果写入目标表；
- 向HDFS导入数据时，最终要的是确保访问的数据源是一致的，而从数据库中并行读取诗句的Map任务分布运行在不同的进程中，因此不可能共享同一个数据库事务。保持一致性的最好方法是导入时不允许运行任何对表中现有数据进行更新的过程；
导出由多个线程并行执行。每个线程使用一个连接，单独的连接到数据库；这些线程之间有独立的事务。

Sqoop使用多行INSERT语法为每个语句插入最多100条记录。每100个语句事务提交一次，也就是每10,000行提交一次。这个机制避免了事务缓冲区无限制地增长导致内存不足的情况。

因此，导出的整个过程不具有原子性，每条数据还是具有原子性的。在导出完成之前，已经导出的部分是可见的。
如果导出mapper任务因这些或其他原因而失败，则会导致导出作业失败。导出失败的结果未定义。每个导出mapper任务都在单独的事务中运行。此外，个别mapper定期执行提交当前事务。如果任务失败，当前事务将被回滚。任何先前提交的事务将在数据库中保持持久，从而导致部分完成的导出。

